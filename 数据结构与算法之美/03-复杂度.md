### 前言

​		现在让我们穿越回复杂度分析的概念和方法产生之前，穿越回算法和数据结构产生之前，一切刚刚起步，**此时的矛盾是cpu运算能力的低下、存储容量的缺乏与日益增长的数据量与日益复杂的需求的矛盾**，于是算法与数据结构应运而生。

​		算法与数据结构解决的问题就是（时间）快，（空间）省。但是会出现一种情况，你说你的算法快，我说我的算法快，那我们到底应该选谁的呢？虽说实践是检验真理的唯一标准，就让两个实现去实际运行一下，但是，想法很美好，实际操作的时候会有如下几个问题：

		1.  如果此时有100个算法，是不是团队就需要实现这100个，然后一一对比，你的成本没办法控制；
		2.  你说我可以参照别人的测试结果，但是运行的硬件会对效率产生影响，数据的规模和数据的状态都对结果有影响，你如果要通过实操来全部复现这些情况，一是你的成本没办法控制，二是场景没办法全部罗列。

基于此，一种理论上来衡量算法的快和省的需求迫在眉睫。

这就是复杂度分析的背景。



### 复杂度分析的工作

>  如果一段代码的运行时间不随着处理数据量的改动而变化，那么这种不在我们分析的范围内，它可能快，可能慢，但对我而言，它的时间是确定的，我们要分析的是数据量大到一定程度之后，代码运行时间的一个变化趋势，这就是渐进时间复杂度，即时间复杂度。

​		如果让你来设计一个复杂度分析理论你要面对哪些问题呢？ 我们以时间复杂度为例，假设我们有如下一段代码：

```java

 int cal(int n) {
   int sum = 0;
   int i = 1;
   for (; i <= n; ++i) {
     sum = sum + i;
   }
   return sum;
 }
```

cpu做的事情就是读取指令，读取数据，运算，写入数据。就java而言，每一行代码其实对应的cpu指令可能是多条，这个取决于编译器和cpu支持的指令集，而且每条指令的执行也不是串行的，可能是并行的，而且还会进行各种优化，这些对我们而言都是会一直变化的，而且我们没办法得知一行代码到达对应了多少条指令，如果按照这个思路走下去，死路一条。



以上就决定了我们的复杂度分析结果一定是一个**不精确**的结果。

如果不是精确的结果，以cpu的执行速度，我们可以假定每行代码的执行时间是一样的，定为unit_time.

那运行的时间就可以转换成代码被执行的次数。

以上文的代码为例，执行的次数就是 1 + 1 + n + 1= n+3; 所以这段代码的执行时间就是(n+3)unit_time.如果要比较两个代码的运行时间的快慢，就是比较unit_time前面系数的大小。

我们用T(n) 表示当数据量是n的时候，程序执行的耗时，那么这里的T(n) = n+3;

我们用unit_time屏蔽了代码运行的宿主机器，和cpu编译器等硬件带来的影响。除了这些之外，我们要处理的数据量的多少和数据的状态（比如是否有序等）都会对我们的运行时间造成影响。

我们的数据量可以简要的概括为上面的n，其实这是一个函数，随着数据大小（自变量）n增加而增加的一个变化趋势，且两者一定是正相关的，即自变量越大，执行的时间越长，我们用**O记号**来描述这种正相关的关系，即T(n) = O(n+3);当数据量很大的时候，我们才需要用到算法分析，比如就200条数据，我们用什么都无所谓，而一旦n大起来，那那些低阶的表达式和常量都可以省略，上面的表达式可简化为T(n) = O(n);

这里会有一个问题，如果我的常量非常大，你不可能说在你的代码中有数十亿行代码这个也能忽略吗，我们理论分析的时候可以忽略，因为当我们理论分析的时候，n的值可能都是趋近10w，100w，1000w，甚至是上亿。但是实际运行的时候可能有差异。

我们用大O记号解决了数据量对算法运行时长的影响，那数据的状态呢？那就是接下来要介绍的四种时间复杂度。

1. 最好情况下的时间复杂度
2. 最坏情况下的时间复杂度
3. 平均情况下的时间复杂度
4. 均摊时间复杂度

```java

// n表示数组array的长度
int find(int[] array, int n, int x) {
  int i = 0;
  int pos = -1;
  for (; i < n; ++i) {
    if (array[i] == x) pos = i;
  }
  return pos;
}


// n表示数组array的长度
int find(int[] array, int n, int x) {
  int i = 0;
  int pos = -1;
  for (; i < n; ++i) {
    if (array[i] == x) {
       pos = i;
       break;
    }
  }
  return pos;
}

```

第一种代码的时间复杂度就是数组的大小n；

第二段代码的时间复杂度就和实际的数据情况相关了，如果数组的第一个数就是我们找的数字，那就只循环了一次；如果数组中不包含我们要找的数字，那就循环了n次。
前者就是**最好情况时间复杂度**为O(1)。后者就是**最坏情况时间复杂度**为O(n);

这两种都是很极端的情况，一般不作为参考依据，这时候就引入了一个新的概念，就是平均情况时间复杂度；
我们想一下，这种场景下，一共有多少种情况，可能在数组索引的第0个-第n-1，以及不在数组中这n+1中情况，每种情况需要执行的循环的次数分别是1、2、3、4....n、n,我们把这n+1种情况需要循环的次数加起来出于n+1就会得到一个n(n+3)/2(n+1),是一个最高阶是n的表达式，根据上面的分析，这种情况下，时间复杂度T(n)=O(n).其实这个是和我们的最坏时间复杂度是一致的。
但是上面的计算有个瑕疵，就是没使用概率论的知识，比如一个数x，在不在数组中，其实是有概率的，简单情况下，我们可以任务概率是一半一半，而在每个位置出现的概率是1/n,所以上面的计算要改为1*1/2n+2*1/2n+.......+n*1/2n+n*1/2 = （3n+1）/4 也还是O(n)，这个值我们称为**加权平均值**，也叫做**期望值**。所以我们也应该称平均时间复杂度为**加权平均时间复杂度**，或者**期望时间复杂度**。

#### 均摊时间复杂度和摊还分析

```java

 // array表示一个长度为n的数组
 // 代码中的array.length就等于n
 int[] array = new int[n];
 int count = 0;
 
 void insert(int val) {
    if (count == array.length) {
       int sum = 0;
       for (int i = 0; i < array.length; ++i) {
          sum = sum + array[i];
       }
       array[0] = sum;
       count = 1;
    }

    array[count] = val;
    ++count;
 }
```

这个例子我们看出，数组的长度是n,前n次我们都是直接插入的，时间复杂度是O(1)，当数组满再插入的时候，我们需要遍历整个数组，时间复杂度是O(n)，然后周而复始，很有规律1、1、1、1.....n、1、1、1、1......n这种，我们用上文的计算方法来完成计算，这一共是n+1种情况，每种情况对应的概率是一样的为1/n+1,那我们的时间复杂度就变成1/n+1+1/n+1+.....+ n/n+1，最后得到一个常量级别的复杂度。这种分析方式就是摊还分析。 摊还时间复杂度一般出现的场景都是这种有规律出现的，个人感觉用到的概率不是很大。



```java

// 全局变量，大小为10的数组array，长度len，下标i。
int array[] = new int[10]; 
int len = 10;
int i = 0;

// 往数组中添加一个元素
void add(int element) {
   if (i >= len) { // 数组空间不够了
     // 重新申请一个2倍大小的数组空间
     int new_array[] = new int[len*2];
     // 把原来array数组中的数据依次copy到new_array
     for (int j = 0; j < len; ++j) {
       new_array[j] = array[j];
     }
     // new_array复制给array，array现在大小就是2倍len了
     array = new_array;
     len = 2 * len;
   }
   // 将element放到下标为i的位置，下标i加一
   array[i] = element;
   ++i;
}
```

上述代码的时间复杂度是多少呢？

一开始的十次是1，然后是10次，然后后面的20次是1，然后是20，然后后面四十次的是1，接着是40；假设我们的最终的数据量大小是n.

我们一定可以拆开成 10+20+40+80+160+....+k=n;这个k是随着n的大小变化而变化的，但是我们发现这个时间复杂度出现的很有规律，无论你的k为多少，你的前面一定有k次的时间复杂度为1，都可以均分给前面出现的k次复杂度是O(1)的，所以时间复杂度是O(1)

