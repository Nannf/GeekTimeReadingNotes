### 前言

​		现在让我们穿越回复杂度分析的概念和方法产生之前，穿越回算法和数据结构产生之前，一切刚刚起步，**此时的矛盾是cpu运算能力的低下、存储容量的缺乏与日益增长的数据量与日益复杂的需求的矛盾**，于是算法与数据结构应运而生。

​		算法与数据结构解决的问题就是（时间）快，（空间）省。但是会出现一种情况，你说你的算法快，我说我的算法快，那我们到底应该选谁的呢？虽说实践是检验真理的唯一标准，就让两个实现去实际运行一下，但是，想法很美好，实际操作的时候会有如下几个问题：

		1.  如果此时有100个算法，是不是团队就需要实现这100个，然后一一对比，你的成本没办法控制；
  		2.  你说我可以参照别人的测试结果，但是运行的硬件会对效率产生影响，数据的规模和数据的状态都对结果有影响，你如果要通过实操来全部复现这些情况，一是你的成本没办法控制，二是场景没办法全部罗列。

基于此，一种理论上来衡量算法的快和省的需求迫在眉睫。

这就是复杂度分析的背景。



### 复杂度分析的工作

​		如果让你来设计一个复杂度分析理论你要面对哪些问题呢？ 我们以时间复杂度为例，假设我们有如下一段代码：

```java

 int cal(int n) {
   int sum = 0;
   int i = 1;
   for (; i <= n; ++i) {
     sum = sum + i;
   }
   return sum;
 }
```

cpu做的事情就是读取指令，读取数据，运算，写入数据。就java而言，每一行代码其实对应的cpu指令可能是多条，这个取决于编译器和cpu支持的指令集，而且每条指令的执行也不是串行的，可能是并行的，而且还会进行各种优化，这些对我们而言都是会一直变化的，而且我们没办法得知一行代码到达对应了多少条指令，如果按照这个思路走下去，死路一条。

以上就决定了我们的复杂度分析结果一定是一个**不精确**的结果。

如果不是精确的结果，以cpu的执行速度，我们可以假定每行代码的执行时间是一样的，定为unit_time.

那运行的时间就可以转换成代码被执行的次数。

以上文的代码为例，执行的次数就是 1 + 1 + n + 1= n+3; 所以这段代码的执行时间就是(n+3)unit_time.

