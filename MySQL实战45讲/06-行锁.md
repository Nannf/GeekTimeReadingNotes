### 行锁

#### 行锁的实现位置

存储引擎，之前MySQL自带的存储引擎MyISAM是不支持的，如果需要改一张表的数据，需要锁表，代价比较大。

> MyISAM被InnoDB替换的两个原因：1. 不支持事务；2，不支持行锁。



#### 行锁的定义

线程在修改表中一个记录的时候，会先申请锁，后续对改行的修改会被阻塞，直到事务结束，这种就是**两阶段锁**。

> 这边有个潜在的优化，我们对一个热点行数据的修改，尽量放在事务的最后，这样我们锁住它的时间会短一些，方便别的事务对其进行操作。

**注意这里的释放时机，不是修改完成，而是事务执行完成，如果有长事务的存在，还是会有那种后续更新全部失败的情况。**



#### 死锁

因为一个事务的修改不一定只修改一条记录，这种就极可能出现，A事务先修改了记录一，接着修改记录二，而B事务是恰好相反的。

这就意味着他们可能出现死锁。

解决方案主要是有两个：

一个是设置超时时间，如果事务运行超时了，就直接回滚之前的操作，这个需要客户端进行重试处理。这种的问题是，超时的时间不好设置，因为如果超时时间过短，我们就会把一些正常的锁等待给回滚了；如果超时时间过长，而他确实存在死锁，这时候再回滚，对业务而言不可接受（MySQL的默认超时时间是50s）;

还有一种就是做死锁检测，比如没有一个事务，~~在查询之前都会去检测我的加入会不会导致死锁~~，这边其实是当自己需要等待的时候才会去分析，这需要分析所有事务的锁依赖关系，如果有1000个事务同时修改一行，第一个运行的事务不用检测，后面的应该是0+1+2+....+999，等差数列求和，复杂度是O(N^2),比较耗时，正常的情况是，事务一旦多起来，会把CPU直接拉满，但是实际执行的事务可能就很少。



为了防止这种检测导致CPU拉满的问题，有的是直接把检测给关了，但是一旦关了，会存在大量的超时，这在业务上是无法接受的；如果不关就是检测到，然后回滚事务，客户端重试。

还有就是控制操作的并发度，把死锁检测提到数据库之前做，引入中间件，但是还是要依赖具体的业务。



最后，我给你留下一个问题吧。如果你要删除一个表里面的前 10000 行数据，有以下三种方法可以做到：第一种，直接执行 delete from T limit 10000;第二种，在一个连接中循环执行 20 次 delete from T limit 500;第三种，在 20 个连接中同时执行 delete from T limit 500。你会选择哪一种方法呢？为什么呢？

第一种会直接锁前10000行，直到事务提交。

第二种是每次锁500行，在每次执行的时候其他的数据都是可以被访问的。

第三种会有问题，第一个事务锁住了前500行，后面的事务全部都死锁超时，会一直检测。

我用第二种。



